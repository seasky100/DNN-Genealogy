[
    {
        "ID": "alexNet",
        "models": ["alexNet"],
        "fullname": "alexNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [],
        "architecture": [
            "streamlined"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with momentum",
            "weight decay",
            "standard relu",
            "random normal",
            "cross entropy"
        ],
        "citation": "18438",
        "date": "2012.12.03"
    },
    {
        "ID": "VGG",
        "models": ["VGG19"],
        "fullname": "VGG",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "increasing depth using an architecture with very small (3x3) convolution filters",
                "ID": "alexNet",
                "link_info_s": "small filter size & deeper",
                "link_category": "streamlined=>streamlined"
            }
        ],
        "architecture": [
            "streamlined"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with momentum",
            "weight decay",
            "standard relu",
            "xavier normal",
            "cross entropy"
        ],
        "citation": "8205",
        "date": "2014.09.04"
    },
    {
        "ID": "highwayNets",
        "fullname": "highwayNets",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "introduce gated skip connections to train extremely deep models directly from scratch",
                "ID": "VGG",
                "link_info_s": "gated skip connections",
                "link_category": "streamlined=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "SGD with momentum",
            "he normal",
            "cross entropy"
        ],
        "citation": "320",
        "date": "2015.11.03"
    },
    {
        "ID": "inception",
        "models": ["inception_v3"],
        "fullname": "inception",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "introduce multi-brach inception module=>less computation cost",
                "ID": "VGG",
                "link_info_s": "inception module",
                "link_category": "streamlined=>multi-branch"
            }
        ],
        "architecture": [
            "multi-branch"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with momentum",
            "weight decay",
            "standard relu",
            "data augmentation"
        ],
        "citation": "5591",
        "date": "2014.09.17"
    },
    {
        "ID": "denseNet",
        "models": ["denseNet_121", "denseNet_169", "denseNet_201"],
        "fullname": "Densely Connected Networks",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "dense residual connection; use concate instead of adding to combine mult-braches",
                "ID": "resNet",
                "link_info_s": "dense residual connections",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with nesterov momentum",
            "weight decay",
            "he_normal",
            "data augmentation"
        ],
        "citation": "378",
        "date": "2016.08.25"
    },
    {
        "ID": "xception",
        "models": ["xception"],
        "fullname": "",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "the inception modules are replaced with depthwise separable convolutions to improve parameter efficiency",
                "ID": "inception",
                "link_info_s": "depthwise seperable conv",
                "link_category": "multi-branch=>multi-branch+depthwise separable conv"
            }
        ],
        "architecture": [
            "multi-branch",
            "depthwise separable conv"
        ],
        "training": [
            "SGD"
        ],
        "citation": "114",
        "date": "2016.10.07"
    },
    {
        "ID": "squeezeNet",
        "models": ["sequeezeNet"],
        "fullname": "squeezeNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "exploiting the split-transform-merge strategy in an easy, extensible way",
                "ID": "inception",
                "link_info_s": "split-transfer-merge",
                "link_category": "multi-branch=>multi-branch"
            },
            {
                "link_info_l": "same performance with 50x less parameters",
                "ID": "alexNet",
                "link_info_s": "50x less parameters",
                "link_category": "streamlined=>multi-branch"
            }
        ],
        "architecture": [
            "multi-branch"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with momentum",
            "weight decay"
        ],
        "citation": "264",
        "date": "2016.02.24"
    },
    {
        "ID": "DPN",
        "fullname": "Dual Path Networks",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "combine the connections in DenseNet and ResNet",
                "ID": "denseNet",
                "link_info_s": "combine",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "combine the connections in DenseNet and ResNet",
                "ID": "resNet",
                "link_info_s": "combine",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "sgd",
            "batch normalization"
        ],
        "citation": "31",
        "date": "2017.06.06"
    },
    {
        "ID": "inception_resNet",
        "models": ["inception_resNet"],
        "fullname": "inception_resNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "combine resnet with inception",
                "ID": "resNet",
                "link_info_s": "combine",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "combine resnet with inception",
                "ID": "inception",
                "link_info_s": "combine",
                "link_category": "multi-branch=>multi-branch"
            }
        ],
        "architecture": [
            "multi-branch",
            "skip connections"
        ],
        "training": [
            "SGD",
            "RMSprop",
            "scaling of the residuals",
            "dropout",
            "weight decay"
        ],
        "citation": "480",
        "date": "2016.02.23"
    },
    {
        "ID": "mobileNet",
        "models": ["mobileNet"],
        "fullname": "moblieNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "apply the same streamlined architecture; add batch normalization; replace normal conv blocks with depthwise seperable conv blocks",
                "ID": "VGG",
                "link_info_s": "same streamlined architecture",
                "link_category": "streamlined=>streamlined"
            },
            {
                "link_info_l": "mobileNet and squeezeNet both aim to build small networks; MobileNet has better performance and less computation cost compared with SqueezeNet with the same size.",
                "ID": "squeezeNet",
                "link_info_s": "smaller",
                "link_category": "multi-branch=>streamlined"
            },
            {
                "link_info_l": "borrow the depthwise seperable convolutions used in xception",
                "ID": "xception",
                "link_info_s": "depthwise seperable conv",
                "link_category": "depthwise separable conv=>depthwise separable conv"
            }
        ],
        "architecture": [
            "depthwise separable conv",
            "streamlined"
        ],
        "training": [
            "SGD",
            "RMSprop"
        ],
        "citation": "149",
        "date": "2017.04.17"
    },
    {
        "ID": "RIR",
        "fullname": "ResNet in ResNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "proposed a new deep dual-stream architecture; generalizes ResNets and standard CNNs",
                "ID": "resNet",
                "link_info_s": "generalizes",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "batch normalization",
            "SGD with momentum",
            "weight decay",
            "dropout",
            "data augmentation"
        ],
        "citation": "62",
        "date": "2016.05.25"
    },
    {
        "ID": "WRN",
        "models": ["WRN_16_4", "WRN_28_10"], 
        "fullname": "Wide ReseNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "decrease depth and increase width in ResNet; the first DNN that successfully combines dropout with batch normalization",
                "ID": "resNet",
                "link_info_s": "wider and shallower",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "batch normalization",
            "SGD with momentum",
            "weight decay",
            "dropout"
        ],
        "citation": "285",
        "date": "2016.05.23"
    },
    {
        "ID": "mixNet",
        "fullname": "MixNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "generalize the connections in ResNet, DenseNet and DPN",
                "ID": "resNet",
                "link_info_s": "generalize",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "generalize the connections in ResNet, DenseNet and DPN",
                "ID": "denseNet",
                "link_info_s": "generalize",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "generalize the connections in ResNet, DenseNet and DPN",
                "ID": "DPN",
                "link_info_s": "generalize",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "SGD with momentum",
            "weight decay",
            "dropout",
            "he normal"
        ],
        "citation": "0",
        "date": "2018.02.06"
    },
    {
        "ID": "fractalNet",
        "models": ["fractalNet_40"],
        "fullname": "fractalNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "train very deep networks without residual connections",
                "ID": "resNet",
                "link_info_s": "deep without residual",
                "link_category": "skip connections=>multi-branch"
            },
            {
                "link_info_l": "exploiting the split-transform-merge strategy in an easy, extensible way",
                "ID": "inception",
                "link_info_s": "split-transfer-merge",
                "link_category": "multi-branch=>multi-branch"
            }
        ],
        "architecture": [
            "multi-branch"
        ],
        "training": [
            "SGD",
            "drop path",
            "batch normalization",
            "SGD with momentum",
            "weight decay",
            "dropout"
        ],
        "citation": "71",
        "date": "2016.05.24"
    },
    {
        "ID": "polyInception",
        "fullname": "polyNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "generalize the additive combination in Inception residual units via various forms of polynomial compositions; encourages the structural diversity and enhances the expressive power",
                "ID": "inception_resNet",
                "link_info_s": "generalize",
                "link_category": "skip connections+multi-branch=>skip connections+multi-branch"
            }
        ],
        "architecture": [
            "multi-branch",
            "skip connections"
        ],
        "training": [
            "SGD",
            "dropout",
            "SGD with momentum",
            "weight decay",
            "standard relu"
        ],
        "citation": "32",
        "date": "2016.12.17"
    },
    {
        "ID": "diracNet",
        "fullname": "diracNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "initialize parameterize weights as a residual of Dirac function",
                "ID": "resNet",
                "link_info_s": "implicit residual",
                "link_category": "skip connections=>streamlined"
            }
        ],
        "architecture": [
            "streamlined"
        ],
        "training": [
            "SGD",
            "dirac",
            "batch normalization",
            "SGD with momentum",
            "weight decay",
            "dropout"
        ],
        "citation": "4",
        "date": "2017.06.07"
    },
    {
        "ID": "resNet",
        "models": ["resNet_v1_50", "resNet_v1_152", "resNet_v2_56", "resNet_v2_110"],
        "fullname": "Residual Networks",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "introduce residual connections to train very deep networks; use batch normalization instead of dropout; use average pooling for the last layer",
                "ID": "VGG",
                "link_info_s": "residual connection",
                "link_category": "streamlined=>skip connections"
            },
            {
                "link_info_l": "skip connections without gate",
                "ID": "highwayNets",
                "link_info_s": "remove gate",
                "link_category": "skip connections=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "batch normalization",
            "SGD with momentum",
            "weight decay",
            "he_normal"
        ],
        "citation": "5464",
        "date": "2015.12.10"
    },
    {
        "ID": "resNeXt",
        "fullname": "resNeXt",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "similar architeccture; ResNeXt has fewer hyperparameters; ResNeXt is highly modulized",
                "ID": "inception_resNet",
                "link_info_s": "highly modulized",
                "link_category": "skip connections+multi-branch=>skip connections+multi-branch"
            },
            {
                "link_info_l": "add a new dimension called \"cardinality\"",
                "ID": "resNet",
                "link_info_s": "residual connections",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "exploiting the split-transform-merge strategy in an easy, extensible way",
                "ID": "inception",
                "link_info_s": "split-transfer-merge",
                "link_category": "multi-branch=>multi-branch"
            }
        ],
        "architecture": [
            "skip connections",
            "multi-branch"
        ],
        "training": [
            "SGD",
            "pre-activation",
            "batch normalization",
            "SGD with momentum",
            "weight decay"
        ],
        "citation": "134",
        "date": "2016.11.16"
    },
    {
        "ID": "SENet",
        "fullname": "Squeeze-and-Excitation Networks",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "SENet is insipired by mobileNet's manipulation on channels; it has significant performance improvements with slight computional cost",
                "ID": "mobileNet",
                "link_info_s": "manipulate channels",
                "link_category": "streamlined=>skip connections"
            }
        ],
        "architecture": [
            "skip connections"
        ],
        "training": [
            "SGD",
            "SGD with momentum",
            "he normal",
            "batch normalization",
            "dropout",
            "cross entropy"
        ],
        "citation": "48",
        "date": "2017.09.05"
    },
    {
        "ID": "nasNet",
        "models":["nasNet_large", "nasNet_small", "nasNet_cifar"],
        "fullname": "nasNet",
        "application": [
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "borrow the skip connections from ResNet and let neural architecture search (NAS) select from them",
                "ID": "resNet",
                "link_info_s": "borrow operations",
                "link_category": "skip connections=>skip connections"
            },
            {
                "link_info_l": "borrow the multi-brach, 1x7 then 7x1 convolution, 1x3 then 3x1 convolution from Inception and let neural architecture search (NAS) select from it",
                "ID": "inception",
                "link_info_s": "borrow operations",
                "link_category": "multi-branch=>multi-branch"
            },
            {
                "link_info_l": "borrow the depthwise separable conv from mobileNet and let neural architecture search (NAS) select from them",
                "ID": "mobileNet",
                "link_info_s": "borrow operations",
                "link_category": "depthwise separable conv=>depthwise separable conv"
            }
        ],
        "architecture": [
            "multi-branch",
            "skip connections",
            "depthwise separable conv"
        ],
        "training": [
            "SGD",
            "neural architecture search by reinforcement learning",
            "RMSProp",
            "LSR",
            "weight decay",
            "dropout"
        ],
        "citation": "35",
        "date": "2017.12.01"
    },
    {
        "ID": "RNN",
        "fullname": "simple RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD",
            "sgd"
        ],
        "citation": "9047",
        "date": "1990.03.01"
    },
    {
        "ID": "attention",
        "fullname": "RNN with attention mechanism",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Attention mechanism allows  to search relevant parts in the context at each step of the output generation.",
                "ID": "seq2seq",
                "link_info_s": "add attention",
                "link_category": "tree-structured"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "2081",
        "date": "2014.09.01"
    },
    {
        "ID": "seq2seq",
        "fullname": "sequence to sequence",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "introduce a RNN architecture to map squence to sequence",
                "ID": "RNN",
                "link_info_s": "sequence to sequence",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "2829",
        "date": "2014.09.10"
    },
    {
        "ID": "conv seq2seq",
        "fullname": "conv seq2seq",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Introduce a CNN structure to map sequence to sequence;  Compared previous RNN architecture, it has a faster speed with a high accuracy.",
                "ID": "seq2seq",
                "link_info_s": "CNN for high speed",
                "link_category": "gated"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "73",
        "date": "2017.05.08"
    },
    {
        "ID": "ESN",
        "fullname": "echo state networks",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "ESN use reservior computing to set the recurrent weights; Only the output weights need to be learnt.",
                "ID": "RNN",
                "link_info_s": "reservoir computing",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1517",
        "date": "2004.04.02"
    },
    {
        "ID": "ESN with leaky units",
        "fullname": "echo state networks with leaky integration units",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "An extension to echo state networks by adding leaky integration units",
                "ID": "ESN",
                "link_info_s": "extend",
                "link_category": "multiple time scale"
            },
            {
                "link_info_l": "An extension to echo state networks by adding leaky integration units",
                "ID": "leaky units",
                "link_info_s": "use it",
                "link_category": "multiple time scale=>multiple time scale"
            }
        ],
        "architecture": [
            "multiple time scale"
        ],
        "training": [
            "SGD"
        ],
        "citation": "414",
        "date": "2007.04.01"
    },
    {
        "ID": "time skip connections",
        "fullname": "recurrent neural units with time skip connections",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": " To address long-term dependency, it adds direct connections from variables in the distant past to variables in the present.",
                "ID": "RNN",
                "link_info_s": "add long time scale",
                "link_category": "multiple time scale"
            }
        ],
        "architecture": [
            "multiple time scale"
        ],
        "training": [
            "SGD"
        ],
        "citation": "15",
        "date": "1998.10.15"
    },
    {
        "ID": "CW-RNN",
        "fullname": "Clockwork RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Time skip connections add edges, so the units can choose to operate on a long time scale or focus on a short one; Removing connections force units to operate the long time scale.  ",
                "ID": "time skip connections",
                "link_info_s": "force on long time scale",
                "link_category": "multiple time scale=>multiple time scale"
            }
        ],
        "architecture": [
            "multiple time scale"
        ],
        "training": [
            "SGD"
        ],
        "citation": "135",
        "date": "2014.02.14"
    },
    {
        "ID": "leaky units",
        "fullname": "recurrent neural units leaky units",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": " To address long-term dependency, leaky units add linear self connections with weights near one.",
                "ID": "RNN",
                "link_info_s": "linear self connections",
                "link_category": "multiple time scale"
            },
            {
                "link_info_l": "Leaky units are units  with linear self-connections and a weight near one on these connections; Leaky units are more smooth and flexible than skip time connections since it adjust the real-valed weight rather than the integer-valued time skip.",
                "ID": "time skip connections",
                "link_info_s": "smooth and flexible",
                "link_category": "multiple time scale=>multiple time scale"
            }
        ],
        "architecture": [
            "multiple time scale"
        ],
        "training": [
            "SGD"
        ],
        "citation": "900",
        "date": "2013.02.13"
    },
    {
        "ID": "LSTM",
        "fullname": "long short term memory",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "LSTM uses a gated self-loop to learn long-term dependencies",
                "ID": "RNN",
                "link_info_s": "gated units",
                "link_category": "gated"
            },
            {
                "link_info_l": "LSTM and leaky units adopt different methods to accumulate information through time. ",
                "ID": "leaky units",
                "link_info_s": "",
                "link_category": "multiple time scale=>gated"
            }
        ],
        "architecture": [
            "gated"
        ],
        "training": [
            "SGD"
        ],
        "citation": "8136",
        "date": "1997.12.15"
    },
    {
        "ID": "SRU",
        "fullname": "Simple Recurrent Unit",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "in the simple recurrent unit architecture, the majority of computation of each step is independent of the recurrence and can be easily parallelized; simple recurrent unit is as fas as a convolutional layer and 5-10x faster than an optimized LSTM",
                "ID": "LSTM",
                "link_info_s": "train fast",
                "link_category": "gated=>gated"
            }
        ],
        "architecture": [
            "gated"
        ],
        "training": [
            "SGD"
        ],
        "citation": "25",
        "date": "2017.09.08"
    },
    {
        "ID": "GRU",
        "fullname": "gated recurrent unit",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "The gates of a recureent uit is redesigned in GRU, input gate and forget gate are combined into one update gate; GRU improves computation effiency",
                "ID": "LSTM",
                "link_info_s": "redesign gates",
                "link_category": "gated=>gated"
            }
        ],
        "architecture": [
            "gated"
        ],
        "training": [
            "SGD"
        ],
        "citation": "2069",
        "date": "2014.06.03"
    },
    {
        "ID": "LRU",
        "fullname": "lattice recurrent unit",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "LRU decouples the GRU to two dimensions; LRU enables better convergence and accuracy",
                "ID": "GRU",
                "link_info_s": "flows of two dimensions",
                "link_category": "gated=>gated"
            }
        ],
        "architecture": [
            "gated"
        ],
        "training": [
            "SGD"
        ],
        "citation": "20",
        "date": "2017.10.06"
    },
    {
        "ID": "gridLSTM",
        "fullname": "Grid Long Short-term Memory",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "gridLSTM arranges LSTM cells in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data",
                "ID": "LSTM",
                "link_info_s": "multi-dimensions",
                "link_category": "gated=>gated"
            }
        ],
        "architecture": [
            "gated"
        ],
        "training": [
            "SGD"
        ],
        "citation": "150",
        "date": "2015.7.06"
    },
    {
        "ID": "recursive",
        "fullname": "recursive neural network",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Generalize RNN from chain-like structure to tree-like structure; Improve the ability to represent hierarchical information",
                "ID": "RNN",
                "link_info_s": "chain to tree",
                "link_category": "tree-structured"
            }
        ],
        "architecture": [
            "tree-structured"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1050",
        "date": "1990.11.01"
    },
    {
        "ID": "tree-LSTM",
        "fullname": "tree structured long short term memory",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "a generalization of LSTMs from chain-structured to tree-structured network topologies; ; Improve the ability to represent hierarchical information",
                "ID": "LSTM",
                "link_info_s": "chain to tree",
                "link_category": "gated=>gated"
            },
            {
                "link_info_l": "build a recursive neural networks with LSTM cells",
                "ID": "recursive",
                "link_info_s": "a specification",
                "link_category": "tree-structured=>tree-structured"
            }
        ],
        "architecture": [
            "gated",
            "tree-structured"
        ],
        "training": [
            "SGD"
        ],
        "citation": "549",
        "date": "2015.02.28"
    },
    {
        "ID": "DGLSTM",
        "fullname": "depth-gated long short-term memory",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "an extension of LSTM to use a depth gate to connect memory cells of adjacent layers",
                "ID": "LSTM",
                "link_info_s": "add depth gate",
                "link_category": "gated=>gated"
            },
            {
                "link_info_l": "use the stacked architecture; add gated skip connection",
                "ID": "stacked RNN",
                "link_info_s": "borrow",
                "link_category": "stacked=>gated"
            }
        ],
        "architecture": [
            "gated",
            "stacked"
        ],
        "training": [
            "SGD"
        ],
        "citation": "15",
        "date": "2015.08.01"
    },
    {
        "ID": "BRNN",
        "fullname": "bidirectional RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Bidirectional inputs enable the current state to get information from both past and future input.",
                "ID": "RNN",
                "link_info_s": "bidirectional inputs",
                "link_category": "bidirectional=>bidirectional"
            }
        ],
        "architecture": [
            "bidirectional"
        ],
        "training": [
            "SGD"
        ],
        "citation": "984",
        "date": "1997.12.01"
    },
    {
        "ID": "stacked RNN",
        "fullname": "stacked RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "Increase deepth to improve the representational capacity",
                "ID": "RNN",
                "link_info_s": "deeper",
                "link_category": "stacked"
            }
        ],
        "architecture": [
            "stacked"
        ],
        "training": [
            "SGD"
        ],
        "citation": "263",
        "date": "1992.03.01"
    },
    {
        "ID": "DB-LSTM",
        "fullname": "deep bidirectional LSTM",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "use LSTM; combine LSTM with other methods and successfully use RNN in speech recognition",
                "ID": "LSTM",
                "link_info_s": "borrow",
                "link_category": "gated=>gated"
            },
            {
                "link_info_l": "use BRNN; combine BRNN with other methods and successfully use RNN in speech recognition",
                "ID": "BRNN",
                "link_info_s": "combine with other methods",
                "link_category": "bidirectional=>bidirectional"
            },
            {
                "link_info_l": "use Stacked RNN; combine Stacked RNN with other methods and successfully use RNN in speech recognition",
                "ID": "stacked RNN",
                "link_info_s": "combine with other methods",
                "link_category": "stacked=>stacked"
            }
        ],
        "architecture": [
            "gated",
            "bidirectional",
            "stacked"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1897",
        "date": "2013.03.22"
    },
    {
        "ID": "DT-RNN",
        "fullname": "deep transition RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "use stacked RNN; add MLP to increase the network depth; improve performance",
                "ID": "stacked RNN",
                "link_info_s": "add MLP",
                "link_category": "stacked=>stacked"
            }
        ],
        "architecture": [
            "stacked"
        ],
        "training": [
            "SGD"
        ],
        "citation": "269",
        "date": "2013.12.20"
    },
    {
        "ID": "DT(S)-RNN",
        "fullname": "add skip connections to deep transition RNN",
        "application": [
            "1.2.NLP"
        ],
        "parents": [
            {
                "link_info_l": "add skip connections to deep transition RNN",
                "ID": "DT-RNN",
                "link_info_s": "skip connections",
                "link_category": "stacked=>stacked"
            }
        ],
        "architecture": [
            "stacked"
        ],
        "training": [
            "SGD"
        ],
        "citation": "269",
        "date": "2013.12.20"
    },
    {
        "ID": "R-CNN",
        "fullname": "R-CNN",
        "application": [
            "2.detection"
        ],
        "parents": [],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "6532",
        "date": "2013.11.11"
    },
    {
        "ID": "SPP-Net",
        "fullname": "Spatial Pyramid Pooling Networks",
        "application": [
            "2.detection",
            "1.1.1.general recognition"
        ],
        "parents": [
            {
                "link_info_l": "extract the feature maps from the entire image only once and apply the spatial pyramid pooling on each candidate windows; can run orders of magnitude faster ",
                "ID": "R-CNN",
                "link_info_s": "SPP",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1337",
        "date": "2014.06.18"
    },
    {
        "ID": "fast R-CNN",
        "fullname": "fast R-CNN",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "combine classification and bounding box regression into a multi-task leaning manner; replace the SVM with a softmax layer; improve the detection speed;training is a single-stage using a multi-task loss; ",
                "ID": "R-CNN",
                "link_info_s": "multi-task",
                "link_category": "nan"
            },
            {
                "link_info_l": "combine classification and bounding box regression into a multi-task leaning manner; training is a single-stage using a multi-task loss; improve the detection speed ",
                "ID": "SPP-Net",
                "link_info_s": "single stage training",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "3651",
        "date": "2015.04.30"
    },
    {
        "ID": "faster R-CNN",
        "fullname": "faster R-CNN",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "use convolutional feature maps to compute region proposals with a deep convolutional neural network",
                "ID": "fast R-CNN",
                "link_info_s": "region proposal network",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "5061",
        "date": "2015.06.04"
    },
    {
        "ID": "YOLO",
        "fullname": "YOLO",
        "application": [
            "2.detection"
        ],
        "parents": [],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "2444",
        "date": "2015.06.08"
    },
    {
        "ID": "YOLO_v2",
        "fullname": "YOLO_v2",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "add adding batch on all of the convolutional layers; remove the fully connected layers from YOLO and use anchor boxes to predict bounding boxes",
                "ID": "YOLO",
                "link_info_s": "anchor boxes",
                "link_category": "nan"
            },
            {
                "link_info_l": "use anchor boxes proposed in faster R-CNN",
                "ID": "faster R-CNN",
                "link_info_s": "anchor boxes",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1219",
        "date": "2016.12.25"
    },
    {
        "ID": "SSD",
        "fullname": "single shot detector",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "simultaneously predicting the bounding box and the class as it processes the image; no object proposal network",
                "ID": "faster R-CNN",
                "link_info_s": "multi-task",
                "link_category": "nan"
            },
            {
                "link_info_l": "use a set of default anchor boxes with different aspect ratios and scales to discretize the output space of bounding boxes; predict based on multiple feature maps with different resolutions; predict the offset of boxes",
                "ID": "YOLO",
                "link_info_s": "region proposal",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "2116",
        "date": "2015.12.08"
    },
    {
        "ID": "DSSD",
        "fullname": "deconvolutional single shot detector",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "combine ReseNet with SSD; use deconvolution layers to introduce additional large-scale context and improve accuracy ",
                "ID": "SSD",
                "link_info_s": "deconv",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "132",
        "date": "2017.01.23"
    },
    {
        "ID": "DSOD",
        "fullname": "DSOD",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "learn object detectors without pretrain; use dense layer-wise connection; use stem blocks inspired by inception ",
                "ID": "SSD",
                "link_info_s": "dense & stem",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "48",
        "date": "2017.08.03"
    },
    {
        "ID": "FPN",
        "fullname": "Feature Pyramid Network",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "compute a multi-scale feature representation using a feature pyramid network",
                "ID": "faster R-CNN",
                "link_info_s": "feature pyramid",
                "link_category": "nan"
            },
            {
                "link_info_l": "reuse the higher-resolution maps of the feature hierarchy",
                "ID": "SSD",
                "link_info_s": "feature pyramid",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "531",
        "date": "2016.12.09"
    },
    {
        "ID": "R-FCN",
        "fullname": "R-FCN",
        "application": [
            "2.detection"
        ],
        "parents": [
            {
                "link_info_l": "no costly per-region RoI-wise subnetwork; region-based detector is fully convolutional with almost all computation shared on the entire image ",
                "ID": "faster R-CNN",
                "link_info_s": "fully convolution",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "757",
        "date": "2016.05.20"
    },
    {
        "ID": "FCN",
        "models": ["FCN_VGG_32s"],
        "fullname": "Fully Convolutional Network",
        "application": [
            "3.segmentation"
        ],
        "parents": [],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "6557",
        "date": "2014.12.14"
    },
    {
        "ID": "U-Net",
        "models": ["U-Net"],
        "fullname": "U-Net",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "use concatenation instead of addition to fusion the information of previous tensor and up-sampling tensor; use FC layer to connect encoder and decoder ",
                "ID": "FCN",
                "link_info_s": "add => concate",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "3139",
        "date": ""
    },
    {
        "ID": "SegNet",
        "fullname": "SegNet",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling; significantly smaller in the number of trainable parameters",
                "ID": "FCN",
                "link_info_s": "upsampling with pooling indices",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "20",
        "date": ""
    },
    {
        "ID": "DeconvNet",
        "fullname": "DeconvNet",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "change the convolution layer to deconvolution layer during up-sampling ",
                "ID": "SegNet",
                "link_info_s": "learnable deconv layers",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "20",
        "date": ""
    },
    {
        "ID": "RedNet",
        "fullname": "residual encoder decoder network",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "use convolution layers for down sample and upsample operation; use residual blocks;  propose a \u2018pyramid supervision\u2019 training scheme",
                "ID": "SegNet",
                "link_info_s": "no pooling",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "20",
        "date": "2018.06.04"
    },
    {
        "ID": "RefineNet",
        "fullname": "RefineNet",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "both use residual blocks; propose chained residual pooling; propose multi-path refinement",
                "ID": "FCN",
                "link_info_s": "multi-path refine",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "20",
        "date": ""
    },
    {
        "ID": "DilatedNet",
        "fullname": "DilatedNet",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "use dilated convolutions; no downsampling or upsampling",
                "ID": "FCN",
                "link_info_s": "dilated conv",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1104",
        "date": "2015.11.23"
    },
    {
        "ID": "PSPNet",
        "fullname": "PSPNet",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "propose pyramid pooling module to aggregate the context; use auxiliary loss as intermediate supervision",
                "ID": "DilatedNet",
                "link_info_s": "pyramid pooling",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "576",
        "date": "2016.12.04"
    },
    {
        "ID": "DeepLab",
        "fullname": "DeepLab",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "use dense Conditional Random Field (CRF) to replace the softmax layer ",
                "ID": "FCN",
                "link_info_s": "CRF",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "411",
        "date": "2014.12.22"
    },
    {
        "ID": "DeepLab_v2",
        "fullname": "DeepLab_v2",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "Use atrous/dilated convolutions;Propose atrous spatial pyramid pooling (ASPP); Use Fully connected CRF",
                "ID": "DeepLab",
                "link_info_s": "atrous conv",
                "link_category": "nan"
            },
            {
                "link_info_l": "Propose atrous spatial pyramid pooling (ASPP); Use fully connected CRF",
                "ID": "DilatedNet",
                "link_info_s": "ASPP + FC-CRF",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "1889",
        "date": "2016.05.12"
    },
    {
        "ID": "GCN",
        "fullname": "global convolutional network",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "use large kernel to improve segmentation tasks",
                "ID": "DeconvNet",
                "link_info_s": "large kernel",
                "link_category": "nan"
            }
        ],
        "architecture": [
            "xxx"
        ],
        "training": [
            "SGD"
        ],
        "citation": "99",
        "date": "2017.03.08"
    },
    {
        "ID": "DeepLab_v3",
        "fullname": "DeepLab_v3",
        "application": [
            "3.segmentation"
        ],
        "parents": [
            {
                "link_info_l": "improve atrous spatial pyramid pooling (ASPP); employ atrous/dilated convolutions in cascade; remove CRF ",
                "ID": "DeepLab_v2",
                "link_info_s": "pyramid pooling",
                "link_category": "nan"
            }
        ],
        "architecture": [],
        "training": [
            "SGD"
        ],
        "citation": "198",
        "date": "2017.06.17"
    },
    {
        "ID": "mask R-CNN",
        "fullname": "mask R-CNN",
        "application": [
            "2.instance segmentation"
        ],
        "parents": [
            {
                "link_info_l": "extend faster r-cnn for instance segmentation",
                "ID": "faster R-CNN",
                "link_info_s": "mask",
                "link_category": "nan"
            }
        ],
        "architecture": [],
        "training": [
            "SGD"
        ],
        "citation": "938",
        "date": "2017.03.20"
    }
]